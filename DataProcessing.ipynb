{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "@Author: Samarth BM\n",
    "\n",
    "@Date: 2021-12-03\n",
    "\n",
    "@Last Modified by: Samarth BM\n",
    "\n",
    "@Title : To read the data from aws s3 and process the data and store the cleaned data to hdfs.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init('/home/samarth/spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv('.env')\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_key=os.getenv(\"ACCESS_KEY_ID\")\n",
    "access_key_secret=os.getenv(\"SECRETE_ACCESS_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading data stored in s3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/08 18:49:00 WARN S3xLoginHelper: The Filesystem URI contains login details. This is insecure and may be unsupported in future.\n"
     ]
    }
   ],
   "source": [
    "df= spark.read.option(\"inferSchema\", \"true\").csv(\"s3a://{0}:{1}@realtimestockdata/StockData.csv\".format(access_key,access_key_secret), header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+------+------+------+------+\n",
      "|                  0|     1|     2|     3|     4|     5|\n",
      "+-------------------+------+------+------+------+------+\n",
      "|2021-12-01 19:31:00|117.03|117.03|117.03|117.03|   150|\n",
      "|2021-12-01 19:03:00|117.43|117.43|117.43|117.43|   250|\n",
      "|2021-12-01 18:26:00| 117.1| 117.1| 117.1| 117.1|   438|\n",
      "|2021-12-01 18:01:00|117.15|117.15|117.15|117.15|   100|\n",
      "|2021-12-01 17:48:00|116.95|116.95|116.95|116.95|   151|\n",
      "|2021-12-01 17:17:00|116.93|116.93|116.93|116.93|   600|\n",
      "|2021-12-01 17:15:00|116.93|116.93|116.93|116.93|   194|\n",
      "|2021-12-01 17:11:00|116.96|116.96|116.96|116.96|   105|\n",
      "|2021-12-01 17:10:00|116.99|116.99|116.99|116.99|   116|\n",
      "|2021-12-01 17:01:00| 117.0| 117.0| 117.0| 117.0|   305|\n",
      "|2021-12-01 16:48:00| 117.2| 117.2| 117.2| 117.2|   100|\n",
      "|2021-12-01 16:39:00|117.01|117.01|117.01|117.01|   514|\n",
      "|2021-12-01 16:26:00|117.43|117.43|117.43|117.43|   125|\n",
      "|2021-12-01 16:17:00| 117.0| 117.0| 117.0| 117.0|   250|\n",
      "|2021-12-01 16:04:00|116.92|116.92|116.92|116.92| 16276|\n",
      "|2021-12-01 16:03:00|116.92|116.92|116.92|116.92| 34102|\n",
      "|2021-12-01 16:02:00|116.92|116.92|116.92|116.92|  1298|\n",
      "|2021-12-01 16:01:00|116.92|116.92|116.67|116.67|435965|\n",
      "|2021-12-01 16:00:00|117.03|117.07|116.85|116.92|143620|\n",
      "|2021-12-01 15:59:00| 117.1|117.11|117.02|117.05| 48469|\n",
      "+-------------------+------+------+------+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Renaming the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+------+------+------+------+\n",
      "|               time|  open|  high|   low| close|volume|\n",
      "+-------------------+------+------+------+------+------+\n",
      "|2021-12-01 19:31:00|117.03|117.03|117.03|117.03|   150|\n",
      "|2021-12-01 19:03:00|117.43|117.43|117.43|117.43|   250|\n",
      "|2021-12-01 18:26:00| 117.1| 117.1| 117.1| 117.1|   438|\n",
      "|2021-12-01 18:01:00|117.15|117.15|117.15|117.15|   100|\n",
      "|2021-12-01 17:48:00|116.95|116.95|116.95|116.95|   151|\n",
      "+-------------------+------+------+------+------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1=df.withColumnRenamed('0','time')\\\n",
    "    .withColumnRenamed('1','open')\\\n",
    "    .withColumnRenamed('2','high')\\\n",
    "    .withColumnRenamed('3','low')\\\n",
    "    .withColumnRenamed('4','close')\\\n",
    "    .withColumnRenamed('5','volume')\n",
    "\n",
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the data types of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df1\\\n",
    "    .withColumn(\"open\",col(\"open\").cast(DoubleType()))\\\n",
    "    .withColumn(\"high\",col(\"high\").cast(DoubleType()))\\\n",
    "    .withColumn(\"low\",col(\"low\").cast(DoubleType()))\\\n",
    "    .withColumn(\"close\",col(\"close\").cast(DoubleType()))\\\n",
    "    .withColumn(\"volume\",col(\"volume\").cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- time: string (nullable = true)\n",
      " |-- open: double (nullable = true)\n",
      " |-- high: double (nullable = true)\n",
      " |-- low: double (nullable = true)\n",
      " |-- close: double (nullable = true)\n",
      " |-- volume: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time      0\n",
       "open      0\n",
       "high      0\n",
       "low       0\n",
       "close     0\n",
       "volume    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data=df2.toPandas()\n",
    "cleaned_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing the cleaned data to hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.write.option(\"header\", True).csv(\"CleanedStockData\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
